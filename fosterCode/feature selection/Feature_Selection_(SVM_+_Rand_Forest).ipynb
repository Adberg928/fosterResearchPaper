{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ8_x7PolFzF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from imblearn.combine import SMOTEENN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsYOBWg4lW9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4398aeb-78e3-45b6-a6a4-1fd7f7ffa2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/Shareddrives/CSCI 461/processed'\n",
            "/content/drive/Shareddrives/CSCI 461/processed\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd 'drive/Shareddrives/CSCI 461/processed'\n",
        "#%ls processed/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsWDuHufmvtx"
      },
      "source": [
        "# Setting up data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j6CPXcCqMF0"
      },
      "outputs": [],
      "source": [
        "# Importing the data\n",
        "data = pd.read_csv(\"combined_ohe_17.csv\") # only 17 yr olds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHTZMBA1qzrS"
      },
      "outputs": [],
      "source": [
        "# fixing education \n",
        "\n",
        "edu_matrix = data[[\"highedcert_associate degree\",\"highedcert_bachelor degree\",\"highedcert_high school or ged\",\n",
        "                  \"highedcert_higher degree\",\"highedcert_none of the above\",\"highedcert_vocational certificate\",\n",
        "                  \"highedcert_vocational license\"]]\n",
        "data[\"edu_categorical\"] = edu_matrix.idxmax(axis=1)\n",
        "\n",
        "educationBinary = \\\n",
        "{   \n",
        "    \"highedcert_associate degree\": 1,\n",
        "    \"highedcert_bachelor degree\": 1,\n",
        "    \"highedcert_high school or ged\": 1,\n",
        "    \"highedcert_higher degree\": 1,\n",
        "    \"highedcert_none of the above\":0,\n",
        "    \"highedcert_vocational certificate\": 1,\n",
        "    \"highedcert_vocational license\": 1\n",
        "}\n",
        "data['edu_binary'] = data[\"edu_categorical\"].map(educationBinary) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_9MEXFylW_y"
      },
      "outputs": [],
      "source": [
        "target='subabuse_yes'\n",
        "data.drop(columns=[\"age\"], inplace=True)\n",
        "data.drop(columns=[\"recnumbr\", \"highedcert_associate degree\",\"highedcert_bachelor degree\",\"highedcert_high school or ged\",\n",
        "                  \"highedcert_higher degree\",\"highedcert_none of the above\",\"highedcert_vocational certificate\",\n",
        "                  \"highedcert_vocational license\",\"edu_categorical\", 'currpte_yes', 'currpte_no','currfte_yes', \n",
        "                   'currfte_no', 'edu_binary','homeless_no','homeless_yes', 'subabuse_no', 'incarc_no', 'incarc_yes', 'edu_categorical'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bidJ0ignmfzK"
      },
      "outputs": [],
      "source": [
        "k=5\n",
        "y = data.loc[:,target]\n",
        "X = data.loc[:,data.columns != target]\n",
        "X=X.reset_index(drop=True)\n",
        "smote_enn = SMOTEENN(random_state=0)\n",
        "X, y = smote_enn.fit_resample(X, y)\n",
        "X.to_csv('variables/X.csv')\n",
        "y.to_csv('variables/y.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DeEGsldmf_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352eeea3-5544-44ed-d74e-db2c0c47f4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROPORTION OF TARGET IN THE ORIGINAL DATA\n",
            "1    0.618311\n",
            "0    0.381689\n",
            "Name: subabuse_yes, dtype: float64\n",
            "\n",
            "\n",
            "SPLIT NO 1\n",
            "TRAINING SET SIZE: 0.8\tTEST SET SIZE: 0.2\n",
            "PROPORTION OF TARGET IN THE TRAINING SET\n",
            "1    0.618246\n",
            "0    0.381754\n",
            "Name: subabuse_yes, dtype: float64\n",
            "PROPORTION OF TARGET IN THE TEST SET\n",
            "1    0.618328\n",
            "0    0.381672\n",
            "Name: subabuse_yes, dtype: float64\n",
            "\n",
            "\n",
            "SPLIT NO 2\n",
            "TRAINING SET SIZE: 0.8\tTEST SET SIZE: 0.2\n",
            "PROPORTION OF TARGET IN THE TRAINING SET\n",
            "1    0.618328\n",
            "0    0.381672\n",
            "Name: subabuse_yes, dtype: float64\n",
            "PROPORTION OF TARGET IN THE TEST SET\n",
            "1    0.618307\n",
            "0    0.381693\n",
            "Name: subabuse_yes, dtype: float64\n",
            "\n",
            "\n",
            "SPLIT NO 3\n",
            "TRAINING SET SIZE: 0.8\tTEST SET SIZE: 0.2\n",
            "PROPORTION OF TARGET IN THE TRAINING SET\n",
            "1    0.618328\n",
            "0    0.381672\n",
            "Name: subabuse_yes, dtype: float64\n",
            "PROPORTION OF TARGET IN THE TEST SET\n",
            "1    0.618307\n",
            "0    0.381693\n",
            "Name: subabuse_yes, dtype: float64\n",
            "\n",
            "\n",
            "SPLIT NO 4\n",
            "TRAINING SET SIZE: 0.8\tTEST SET SIZE: 0.2\n",
            "PROPORTION OF TARGET IN THE TRAINING SET\n",
            "1    0.618328\n",
            "0    0.381672\n",
            "Name: subabuse_yes, dtype: float64\n",
            "PROPORTION OF TARGET IN THE TEST SET\n",
            "1    0.618307\n",
            "0    0.381693\n",
            "Name: subabuse_yes, dtype: float64\n",
            "\n",
            "\n",
            "SPLIT NO 5\n",
            "TRAINING SET SIZE: 0.8\tTEST SET SIZE: 0.2\n",
            "PROPORTION OF TARGET IN THE TRAINING SET\n",
            "1    0.618328\n",
            "0    0.381672\n",
            "Name: subabuse_yes, dtype: float64\n",
            "PROPORTION OF TARGET IN THE TEST SET\n",
            "1    0.618307\n",
            "0    0.381693\n",
            "Name: subabuse_yes, dtype: float64\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n",
        "#data['target'] IS THE VARIABLE USED FOR STRATIFIED SAMPLING.\n",
        "splits = kfold.split(X,y)\n",
        "print(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\\n{y.value_counts() / len(y)}\\n\\n')\n",
        "for n,(train_index,test_index) in enumerate(splits):\n",
        "    print(f'SPLIT NO {n+1}\\nTRAINING SET SIZE: {np.round(len(train_index) / (len(train_index)+len(test_index)),2)}'+\n",
        "          f'\\tTEST SET SIZE: {np.round(len(test_index) / (len(train_index)+len(test_index)),2)}\\nPROPORTION OF TARGET IN THE TRAINING SET\\n'+\n",
        "          f'{y[test_index].value_counts() / len(y[test_index])}\\nPROPORTION OF TARGET IN THE TEST SET\\n'+\n",
        "          f'{y[train_index].value_counts() / len(y[train_index])}\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTfOKKAwl5tq"
      },
      "source": [
        "# Stratified Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSxM2A9Dl5Ag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "27c28cb7-662b-4ea7-f464-5ef72b1ca51e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nfrom xgboost import XGBClassifier\\n\\n# instantiate classifier with default hyperparameters\\nmodel=XGBClassifier(class_weight=\"balanced\")\\n#cm=[0,0,0,0]\\nauc=[]\\nf1=[]\\ny_truth=[]\\ny_pred=[]\\nsplits = kfold.split(X, y)\\nfor n,(train_index, test_index) in enumerate(splits):\\n  model.fit(X.iloc[train_index, :-1], y.iloc[train_index])\\n  pred = model.predict(X.iloc[test_index, :-1])\\n  f=f1_score(y.iloc[test_index], pred)\\n  f1.append(f)\\n  a=roc_auc_score(y.iloc[test_index], pred)\\n  auc.append(a)\\n  print(\"FOLD {} \\n AUC {}\".format(n,a))\\n  # yi = my_cm_dec(y.iloc[test_index,], pred)\\n  # print(yi)\\n  # cm= [cm[i]+ yi[i] for i in range(len(cm))]\\n  for i in y[test_index]: y_truth.append(i)\\n  for i in pred: y_pred.append(i)\\n\\n\\nprint(sum(auc)/5)\\nprint(stdev(auc))\\nprint(sum(f1)/5)\\nprint(stdev(f1))\\nprint(my_cm_dec(y_truth,y_pred))\\n\\nprint(\"Female by Percenct\")\\nprint(my_cm_dec(pd.Series(y_truth).iloc[X.index[X[\\'sex_female\\'] == True].values], pd.Series(y_pred).iloc[X.index[X[\\'sex_female\\'] == True].values]))\\nplt.show()\\nprint(\"Male by Percenct\")\\nprint(my_cm_dec(pd.Series(y_truth).iloc[X.index[X[\\'sex_male\\'] == True].values], pd.Series(y_pred).iloc[X.index[X[\\'sex_male\\'] == True].values]))\\nplt.show()\\n\\nfor i in [\\'South\\', \\'Midwest\\', \"Northeast\", \\'West\\']:\\n  print(\"Breakdown by {}\".format(i))\\n  print(my_cm_dec(pd.Series(y_truth)[region.index[region == i]], pd.Series(y_pred)[region.index[region == i]]))\\n  plt.show()\\n\\n\\nfor i in [\\'amiakn\\',\\'asian\\', \\'blkafram\\', \\'hawaiipi\\', \\'white\\',\\'raceunkn\\', \\'hisorgin\\']:\\n  print(\"Breakdown by {}\".format(i))\\n\\n  print(\"YES - By percent\")\\n  print(my_cm_dec(pd.Series(y_truth).iloc[X.index[X[i+\\'_yes\\'] == True].values], pd.Series(y_pred).iloc[X.index[X[i+\\'_yes\\'] == True].values]))\\n  plt.show()\\n\\n  print(\"NO - by percent\")\\n  print(my_cm_dec(pd.Series(y_truth).iloc[X.index[X[i+\\'_no\\'] == True].values], pd.Series(y_pred).iloc[X.index[X[i+\\'_no\\'] == True].values]))\\n  plt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# instantiate classifier with default hyperparameters\n",
        "model=XGBClassifier(class_weight=\"balanced\")\n",
        "#cm=[0,0,0,0]\n",
        "auc=[]\n",
        "f1=[]\n",
        "y_truth=[]\n",
        "y_pred=[]\n",
        "splits = kfold.split(X, y)\n",
        "for n,(train_index, test_index) in enumerate(splits):\n",
        "  model.fit(X.iloc[train_index, :-1], y.iloc[train_index])\n",
        "  pred = model.predict(X.iloc[test_index, :-1])\n",
        "  f=f1_score(y.iloc[test_index], pred)\n",
        "  f1.append(f)\n",
        "  a=roc_auc_score(y.iloc[test_index], pred)\n",
        "  auc.append(a)\n",
        "  print(\"FOLD {} \\n AUC {}\".format(n,a))\n",
        "  # yi = my_cm_dec(y.iloc[test_index,], pred)\n",
        "  # print(yi)\n",
        "  # cm= [cm[i]+ yi[i] for i in range(len(cm))]\n",
        "  for i in y[test_index]: y_truth.append(i)\n",
        "  for i in pred: y_pred.append(i)\n",
        "\n",
        "\n",
        "print(sum(auc)/5)\n",
        "print(stdev(auc))\n",
        "print(sum(f1)/5)\n",
        "print(stdev(f1))\n",
        "print(my_cm_dec(y_truth,y_pred))\n",
        "\n",
        "print(\"Female by Percenct\")\n",
        "print(my_cm_dec(pd.Series(y_truth).iloc[X.index[X['sex_female'] == True].values], pd.Series(y_pred).iloc[X.index[X['sex_female'] == True].values]))\n",
        "plt.show()\n",
        "print(\"Male by Percenct\")\n",
        "print(my_cm_dec(pd.Series(y_truth).iloc[X.index[X['sex_male'] == True].values], pd.Series(y_pred).iloc[X.index[X['sex_male'] == True].values]))\n",
        "plt.show()\n",
        "\n",
        "for i in ['South', 'Midwest', \"Northeast\", 'West']:\n",
        "  print(\"Breakdown by {}\".format(i))\n",
        "  print(my_cm_dec(pd.Series(y_truth)[region.index[region == i]], pd.Series(y_pred)[region.index[region == i]]))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "for i in ['amiakn','asian', 'blkafram', 'hawaiipi', 'white','raceunkn', 'hisorgin']:\n",
        "  print(\"Breakdown by {}\".format(i))\n",
        "\n",
        "  print(\"YES - By percent\")\n",
        "  print(my_cm_dec(pd.Series(y_truth).iloc[X.index[X[i+'_yes'] == True].values], pd.Series(y_pred).iloc[X.index[X[i+'_yes'] == True].values]))\n",
        "  plt.show()\n",
        "\n",
        "  print(\"NO - by percent\")\n",
        "  print(my_cm_dec(pd.Series(y_truth).iloc[X.index[X[i+'_no'] == True].values], pd.Series(y_pred).iloc[X.index[X[i+'_no'] == True].values]))\n",
        "  plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ppXYYiuNa-H"
      },
      "outputs": [],
      "source": [
        "# data with no service columns\n",
        "data_ns = data.drop(columns=['specedsv_No','specedsv_Yes','ilnasv_No',\n",
        " 'ilnasv_Yes','acsuppsv_No','acsuppsv_Yes','psedsuppsv_No',\n",
        " 'psedsuppsv_Yes','careersv_No','careersv_Yes','emplytrsv_No',\n",
        " 'emplytrsv_Yes','budgetsv_No','budgetsv_Yes','housedsv_No',\n",
        " 'housedsv_Yes','hlthedsv_No','hlthedsv_Yes','famsuppsv_No',\n",
        " 'famsuppsv_Yes','mentorsv_No','mentorsv_Yes','silsv_No','silsv_Yes',\n",
        " 'rmbrdfasv_No','rmbrdfasv_Yes','educfinasv_No','educfinasv_Yes',\n",
        " 'othrfinasv_No','othrfinasv_Yes'], axis=1)\n",
        "\n",
        "# X with no service columns\n",
        "X_ns = X.drop(columns=['specedsv_No','specedsv_Yes','ilnasv_No',\n",
        " 'ilnasv_Yes','acsuppsv_No','acsuppsv_Yes','psedsuppsv_No',\n",
        " 'psedsuppsv_Yes','careersv_No','careersv_Yes','emplytrsv_No',\n",
        " 'emplytrsv_Yes','budgetsv_No','budgetsv_Yes','housedsv_No',\n",
        " 'housedsv_Yes','hlthedsv_No','hlthedsv_Yes','famsuppsv_No',\n",
        " 'famsuppsv_Yes','mentorsv_No','mentorsv_Yes','silsv_No','silsv_Yes',\n",
        " 'rmbrdfasv_No','rmbrdfasv_Yes','educfinasv_No','educfinasv_Yes',\n",
        " 'othrfinasv_No','othrfinasv_Yes'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70UdKWOYOeMD"
      },
      "outputs": [],
      "source": [
        "# data with no states\n",
        "data_ngeo = data.drop(columns = data.filter(regex=(\"st.*\")))\n",
        "\n",
        "# X with no states\n",
        "X_ngeo = X.drop(columns = X.filter(regex=(\"st.*\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUID-3S4JkZ8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def my_cm_dec(y,y_pred):\n",
        "  cm = confusion_matrix(y,y_pred)\n",
        "  n=sum(cm.ravel()[:2])\n",
        "  p=sum(cm.ravel()[2:])\n",
        "  cm_matrix = pd.DataFrame(data=cm, columns=['Predict Negative:0','Predict Positive:1'], \n",
        "                                 index=[ 'Actual Negative:0','Actual Positive:1'])\n",
        "  cm_matrix = cm_matrix.loc[ ['Actual Positive:1','Actual Negative:0'], [ 'Predict Positive:1', 'Predict Negative:0']]\n",
        "  cm_matrix.loc['Actual Positive:1', :] = cm_matrix.loc['Actual Positive:1', :] /p\n",
        "  cm_matrix.loc['Actual Negative:0', :]  = cm_matrix.loc['Actual Negative:0', :] /n\n",
        "  sns.heatmap(cm_matrix, annot=True, cmap='YlGnBu')\n",
        "  plt.show()\n",
        "\n",
        "  cf_matrix = confusion_matrix(y, y_pred)\n",
        "  tp = cf_matrix[1][1]\n",
        "  tn = cf_matrix[0][0]\n",
        "  fp = cf_matrix[0][1]\n",
        "  fn = cf_matrix[1][0]\n",
        "\n",
        "  print(\"False Positive rate: \", round(fp/(fp+tn),2), \" Out of all wrong preds, what % was positive?\")\n",
        "  print(\"False Negative rate: \", round(fn/(fn+tp),2), \" Out of all wrong preds, what % was negative?\")\n",
        "  print(\"Specificity: \", round(tn/(tn+fp),2), \" Out of all ppl who didn't recieve referals, what % was correct?\")\n",
        "  print(\"Sensitivity: \", round(tp/(tp+fn),2),\" Out of all ppl who did receive referals, what % was correct?\")\n",
        "\n",
        "  return cm.ravel()\n",
        "\n",
        "def my_scores(mod, X, y,k=5):\n",
        "  scoring = ['accuracy', 'precision', 'f1', 'recall', 'roc_auc']\n",
        "  scores = cross_validate(mod, X, y, cv=k,scoring=scoring)\n",
        "  print(\"accuracy:\",sum(scores[\"test_accuracy\"])/k)\n",
        "  print(\"precision:\",sum(scores[\"test_precision\"])/k)\n",
        "  print(\"auc:\",sum(scores[\"test_roc_auc\"])/k)\n",
        "  print(\"recall:\",sum(scores[\"test_recall\"])/k)\n",
        "  print(\"f1:\",sum(scores[\"test_f1\"])/k)\n",
        "  results = [sum(scores[\"test_accuracy\"])/k,\n",
        "             sum(scores[\"test_precision\"])/k,\n",
        "             sum(scores[\"test_roc_auc\"])/k,\n",
        "             sum(scores[\"test_recall\"])/k,\n",
        "             sum(scores[\"test_f1\"])/k\n",
        "             ]\n",
        "  return results\n",
        "\n",
        "def stdev(test_list):\n",
        "  mean = sum(test_list) / len(test_list)\n",
        "  return (sum([((x - mean) ** 2) for x in test_list]) / len(test_list)) ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCSkBuPhnqx7"
      },
      "source": [
        "# Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_s1tCB2KCP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "71448ce7-a53d-48b3-9e51-fe29e4114db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "Successfully installed scikit-learn-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eWkY4lcl5Fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "741ce6bc-1e75-46ef-9123-e501ab9f0782"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d5db75ce9526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# SelectFromModel will select those features which importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_selection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_univariate_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_univariate_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf_classif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_univariate_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf_oneway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStrOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_is_arraylike_not_scalar' from 'sklearn.utils.validation' (/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# SelectFromModel will select those features which importance \n",
        "# is greater than the mean importance of all the features by default, but we can \n",
        "# alter this threshold if we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcJvd21il5Hw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_train, y_train)\n",
        "selected_feat= X_train.columns[(sel.get_support())]\n",
        "len(selected_feat)\n",
        "pd.series(sel.estimator_,feature_importances_,.ravel()).hist()\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gISvUw55TRrL"
      },
      "outputs": [],
      "source": [
        "rnd_forest = pd.DataFrame(columns=[\"Baseline\", \"Feature_Sel\", \"Feature_Sel_ns\", \"Feature_Sel_ngeo\"],\n",
        "                          index = [\"accuracy\", \"precision\",\"auc\",\"recall\",\"f1\" ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erGX1gSVl5KH"
      },
      "outputs": [],
      "source": [
        "# BASELINE PERFORMANCE\n",
        "\n",
        "# instantiate classifier with default hyperparameters\n",
        "rfc=RandomForestClassifier() \n",
        "\n",
        "# make predictions on test set\n",
        "start = time.perf_counter()\n",
        "y_pred = cross_val_predict(rfc, X, y, cv=k)\n",
        "end = time.perf_counter()\n",
        "rnd_forest[\"Baseline\"] = my_scores(rfc,X,y)\n",
        "print(\"TIME:\",(end-start)/60,\"min\")\n",
        "\n",
        "my_cm_dec(y, y_pred) # EXTREMELY DIFFERENT TO WHAT'S IN THE SLIDES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4cPc5Vl5MU"
      },
      "outputs": [],
      "source": [
        "start = time.perf_counter()\n",
        "\n",
        "# FEATURE SELECTION (all data (includes services, states))\n",
        "\n",
        "# selecting features\n",
        "sel = SelectFromModel(RandomForestClassifier(max_features=15))\n",
        "sel.fit(X, y)\n",
        "selected_feat= X.columns[(sel.get_support())]\n",
        "print(\"Num. features selected:\",len(selected_feat))\n",
        "print(\"SELECTED FEATURES: \")\n",
        "display(selected_feat)\n",
        "#pd.Series(sel.estimator_.feature_importances_.ravel()).hist()\n",
        "\n",
        "# model performance with selected features\n",
        "X_selected = X.filter(items=selected_feat)\n",
        "y_pred = cross_val_predict(rfc, X_selected, y, cv=k)\n",
        "print(\"Scores:\")\n",
        "rnd_forest[\"Feature_Sel\"] = my_scores(rfc,X_selected,y)\n",
        "my_cm_dec(y, y_pred)\n",
        "\n",
        "end = time.perf_counter()\n",
        "print(\"TIME:\",(end-start)/60,\"min\")\n",
        "# should we be cross validating? how? manually?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ZKvUx8e7Z6"
      },
      "outputs": [],
      "source": [
        "!pip install Boruta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou4nrr2TecbE"
      },
      "outputs": [],
      "source": [
        "from boruta import BorutaPy\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# instantiate random forest\n",
        "forest = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
        "\n",
        "# fit boruta\n",
        "boruta_selector = BorutaPy(forest, n_estimators = 'auto', random_state = 0)\n",
        "boruta_selector.fit(np.array(X), np.array(y))\n",
        "\n",
        "# store results\n",
        "boruta_ranking = boruta_selector.ranking_\n",
        "selected_features = np.array(X.columns)[boruta_ranking <= 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whk7muIgl5Op"
      },
      "outputs": [],
      "source": [
        "start = time.perf_counter()\n",
        "\n",
        "# FEATURE SELECTION (without services)\n",
        "\n",
        "# selecting features\n",
        "sel = SelectFromModel(RandomForestClassifier(max_features=15))\n",
        "sel.fit(X_ns, y)\n",
        "selected_feat= X_ns.columns[(sel.get_support())]\n",
        "print(\"Num. features selected:\",len(selected_feat))\n",
        "print(\"SELECTED FEATURES: \")\n",
        "display(selected_feat)\n",
        "#pd.Series(sel.estimator_.feature_importances_.ravel()).hist()\n",
        "\n",
        "# model performance with selected features\n",
        "X_selected = X.filter(items=selected_feat)\n",
        "y_pred = cross_val_predict(rfc, X_selected, y, cv=k)\n",
        "print(\"Scores:\")\n",
        "rnd_forest[\"Feature_Sel_ns\"] = my_scores(rfc,X_selected,y)\n",
        "my_cm_dec(y, y_pred)\n",
        "\n",
        "end = time.perf_counter()\n",
        "print(\"TIME:\",(end-start)/60,\"min\")\n",
        "\n",
        "# when we drop services state becomes more prevalent (proxy for services offered)\n",
        "# look at corr between services offered and states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-sYRj5DlXCh"
      },
      "outputs": [],
      "source": [
        "start = time.perf_counter()\n",
        "\n",
        "# FEATURE SELECTION (without states)\n",
        "\n",
        "# selecting features\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_ngeo, y)\n",
        "selected_feat= X_ngeo.columns[(sel.get_support())]\n",
        "print(\"Num. features selected:\",len(selected_feat))\n",
        "print(\"SELECTED FEATURES: \")\n",
        "display(selected_feat)\n",
        "#pd.Series(sel.estimator_.feature_importances_.ravel()).hist()\n",
        "\n",
        "# model performance with selected features\n",
        "X_selected = X.filter(items=selected_feat)\n",
        "y_pred = cross_val_predict(rfc, X_selected, y, cv=k)\n",
        "print(\"Scores:\")\n",
        "rnd_forest[\"Feature_Sel_ngeo\"] = my_scores(rfc,X_selected,y)\n",
        "my_cm_dec(y, y_pred)\n",
        "\n",
        "end = time.perf_counter()\n",
        "print(\"TIME:\",(end-start)/60,\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9LTPmhEWdIe"
      },
      "outputs": [],
      "source": [
        "display(rnd_forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW8HqZjcaHIM"
      },
      "outputs": [],
      "source": [
        "# DROPPING \"NO\" --> TEST\n",
        "\n",
        "#service_df = service_df[service_df.columns.drop(list(service_df.filter(regex='_No')))]\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "# FEATURE SELECTION (all data (includes services, states))\n",
        "\n",
        "# selecting features\n",
        "X_yes = X[X.columns.drop(list(X.filter(regex='_no')))]\n",
        "X_yes = X_yes[X_yes.columns.drop(list(X_yes.filter(regex='_female')))]\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_yes, y)\n",
        "selected_feat= X_yes.columns[(sel.get_support())]\n",
        "len(selected_feat)\n",
        "print(\"SELECTED FEATURES: \")\n",
        "display(selected_feat)\n",
        "#pd.Series(sel.estimator_.feature_importances_.ravel()).hist()\n",
        "\n",
        "# model performance with selected features\n",
        "X_selected = X_yes.filter(items=selected_feat)\n",
        "y_pred = cross_val_predict(rfc, X_selected, y, cv=k)\n",
        "print(\"Scores:\")\n",
        "my_cm_dec(y, y_pred)\n",
        "\n",
        "end = time.perf_counter()\n",
        "print(\"TIME:\",(end-start)/60,\"min\")\n",
        "# should we be cross validating? how? manually?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRroqwGYntMj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rnd_forest_d = rnd_forest.copy()\n",
        "rnd_forest_d[\"idx\"] = rnd_forest.index\n",
        "\n",
        "ax = rnd_forest_d.plot(x=\"idx\", y=rnd_forest.columns, kind=\"bar\", rot=0, ylim=(0.75,0.97), grid=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNUKsdybY2L8"
      },
      "outputs": [],
      "source": [
        "# Other notes:\n",
        "  # Maybe should try rand forest reg? so we can get coeffieints for features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZGgrswUY6Te"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rkyO5BYY2OR"
      },
      "outputs": [],
      "source": [
        "# EXPLORING SVC (linear bc want coef)\n",
        "\n",
        "# import SVC classifier\n",
        "#from sklearn.svm import linearSVC\n",
        "from sklearn import svm\n",
        "\n",
        "# instantiate classifier with default hyperparameters\n",
        "lsvc= svm.SVC(kernel = \"linear\")\n",
        "\n",
        "# make predictions on test set\n",
        "start = time.perf_counter()\n",
        "y_pred = cross_val_predict(lsvc, X, y, cv=k)\n",
        "end = time.perf_counter()\n",
        "print(\"TIME:\",(end-start)/60,\"min\")\n",
        "\n",
        "# print confusion matrix\n",
        "#my_cm(y, y_pred)\n",
        "\n",
        "# print coef\n",
        "lsvc.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibn-pak9xH3p"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "lin_svc = pd.DataFrame(columns=[\"feature\",\"coef\"])\n",
        "lin_svc[\"feature\"] = X.columns\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X, y)\n",
        "lin_svc[\"coef\"] = model.coef_[0,:]\n",
        "lin_svc[\"abs coef\"] = abs(lin_svc[\"coef\"])\n",
        "\n",
        "lin_svc.sort_values(by=\"abs coef\",inplace=True, ascending = False)\n",
        "lin_svc.head(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPbm0gbCU2E4"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "lin_svc = pd.DataFrame(columns=[\"feature\",\"coef\"])\n",
        "lin_svc[\"feature\"] = X_ngeo.columns\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X_ngeo, y)\n",
        "lin_svc[\"coef\"] = model.coef_[0,:]\n",
        "lin_svc[\"abs coef\"] = abs(lin_svc[\"coef\"])\n",
        "\n",
        "lin_svc.sort_values(by=\"abs coef\",inplace=True, ascending = False)\n",
        "lin_svc.head(30)\n",
        "\n",
        "# even when don't have states the services aren't super important...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTjdQmKsdnrS"
      },
      "outputs": [],
      "source": [
        "# keeping only services that habe \"yes\" (making no default or didn't answer default)\n",
        "\n",
        "X_yes = X.filter(regex='_yes')\n",
        "X_yes[\"female_yes\"] = X[\"sex_female\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV0_RK8EePi8"
      },
      "outputs": [],
      "source": [
        "# yes baseline change, no states\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "lin_svc = pd.DataFrame(columns=[\"feature\",\"coef\"])\n",
        "lin_svc[\"feature\"] = X_yes.columns\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X_yes, y)\n",
        "lin_svc[\"coef\"] = model.coef_[0,:]\n",
        "lin_svc[\"abs coef\"] = abs(lin_svc[\"coef\"])\n",
        "\n",
        "lin_svc.sort_values(by=\"abs coef\",inplace=True, ascending = False)\n",
        "display(lin_svc.drop(\"abs coef\", axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHuT5jFsgCXT"
      },
      "outputs": [],
      "source": [
        "# yes baseline change, with states\n",
        "\n",
        "state_dummies = data.filter(regex=(\"st.*\"))\n",
        "X_yes_st = pd.concat([X_yes,state_dummies],axis=1,join=\"inner\")\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "lin_svc = pd.DataFrame(columns=[\"feature\",\"coef\"])\n",
        "lin_svc[\"feature\"] = X_yes_st.columns\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X_yes_st, y)\n",
        "lin_svc[\"coef\"] = model.coef_[0,:]\n",
        "lin_svc[\"abs coef\"] = abs(lin_svc[\"coef\"])\n",
        "\n",
        "lin_svc.sort_values(by=\"abs coef\",inplace=True, ascending = False)\n",
        "lin_svc.drop(\"abs coef\", axis=1).head(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39RTmQQ_dn5U"
      },
      "outputs": [],
      "source": [
        "X_yes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy_SNv5QwYZ8"
      },
      "outputs": [],
      "source": [
        "# SHAP\n",
        "\n",
        "import shap \n",
        "explainer = shap.TreeExplainer(random_forest)\n",
        "\n",
        "explainer = shap.KernelExplainer(knn.predict,X)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "shap.summary_plot(shap_values, X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6jqbazCY2Qg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJdD6xN_Y2Sv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXJZqqKGY2VF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZs9fGl7Y2Xd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pEfPnOJY2Zo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTkm0uNxY2b8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsqYvxDGY2d2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}